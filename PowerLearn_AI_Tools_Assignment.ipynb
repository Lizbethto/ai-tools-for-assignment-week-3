{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# Power Learn Project â€“ AI Assignment\n",
    "# Author: Omope Elizabeth\n",
    "# ===============================\n",
    "\n",
    "# This notebook demonstrates three major AI domains using Python:\n",
    "# 1. Classical Machine Learning with Scikit-learn\n",
    "# 2. Deep Learning with TensorFlow\n",
    "# 3. Natural Language Processing with spaCy\n",
    "\n",
    "# Each section contains clearly commented code to show understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9cbe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# Part 1: Classical ML with Scikit-learn (Iris Dataset)\n",
    "# ===============================\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Decision Tree Classifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Model Evaluation Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# Part 2: Deep Learning with TensorFlow (MNIST Dataset)\n",
    "# ===============================\n",
    "\n",
    "# Import required TensorFlow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the data to improve convergence\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Reshape to fit CNN input format\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Build a Convolutional Neural Network (CNN)\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# Part 3: Natural Language Processing with spaCy\n",
    "# ===============================\n",
    "\n",
    "# Import spaCy for text analysis\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text (Amazon-style product review)\n",
    "text = \"I love my new Samsung Galaxy phone! The camera quality is excellent and battery lasts long.\"\n",
    "\n",
    "# Perform Named Entity Recognition (NER)\n",
    "doc = nlp(text)\n",
    "print(\"Named Entities Found:\")\n",
    "for entity in doc.ents:\n",
    "    print(f\" - {entity.text} ({entity.label_})\")\n",
    "\n",
    "# Simple Rule-based Sentiment Analysis\n",
    "positive_words = [\"love\", \"excellent\", \"great\", \"amazing\", \"good\", \"best\"]\n",
    "negative_words = [\"bad\", \"terrible\", \"poor\", \"awful\", \"worst\"]\n",
    "\n",
    "if any(word in text.lower() for word in positive_words):\n",
    "    sentiment = \"Positive\"\n",
    "elif any(word in text.lower() for word in negative_words):\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "print(\"\\nSentiment Analysis Result:\", sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a974ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# Ethical Reflection\n",
    "# ===============================\n",
    "\n",
    "# Note:\n",
    "# Datasets like MNIST may contain limited diversity, leading to potential bias.\n",
    "# Similarly, sentiment models may misinterpret tone based on context or culture.\n",
    "# To address this, fairness tools (e.g., TensorFlow Fairness Indicators) can be used.\n",
    "# Proper dataset balancing and transparency are key to building ethical AI systems.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
